---
title: "Text Analysis in R"
subtitle: "A Short Introduction"
author: "Henry Overos"
date: "9/14/2020"
output: 
  prettydoc::html_pretty:
    theme: architect
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Text Analysis

One of the nice things about studying politics is that there are plenty of types of political data that we can study. Elections, debates, protests, and all sorts of other major and minor political events all provide information to researchers about the way politics functions. One important aspect of most political phenomenon is text or speech. Debates, campaigns, manifestos, news articles, and tweets are all sources that provide information about actors' views, actions and more. With the improvements and availability of `natural language processing (NLP)` resources, political scientists have sought to utilize these various **texts as data** across the range of subtopics in the field.

There are now some best practices across disciplines about `NLP` as well as a variety of resources that highlight the utility and problems with studying politics using large amounts of text data.

This document is intended as an introduction for those who are new to the world of **text as data**, particularly in political science. For the most part all you will need to know for this document is the basics of `R` (and some knowledge of `tidyverse` will be useful but is not necessary).

## Text as Data (in R)

In computer lingo, pure text or characters are considered a *type* of object. Major object types in vector-based computer languages are:

- character (strings)
- numeric
- logical (`TRUE`, `FALSE`, etc.)
- factor

The character class is also referred to as a string

```{r string}
rm(list = ls())
x <- c("The lady", "doth protest too much", ",methinks.")
x

hllowrld <- "Hello, world!"
hllowrld
```

There are no numerical attachments to character strings - i.e. they are not labels like factors. Factors are actually numbers where text labels attached to them. Here, we are speaking of pure text. This means that, on its own, feeding text into `R` does very little for us. We can search it. We can count words as well. But we can't actually explain anything about the text immediately.

### Strings are dumb

You might be thinking, "Wait a minute! How does a computer help me understand text if all it can do is print things other people have written?" That's a question that I didn't ask right away because I wasn't thinking like a computer or data scientist. As a political scientist I already know that text has meaning to it and often I am looking to extract or infer some specific political meaning from the text. But the computer doesn't know that!

This is the most frustrating aspect of `NLP` for someone who is just learning it. For right now I'm just going to mention this up front so that you don't misunderstand what is happening whenever someone says they use "text analysis" methods. Text analysis tools are all means of overcoming problems that computeres make because natural language is too complex for direct analysis. By making assumptions, recognizing patterns in text, or sometimes treating words as numbers and sentences like equations, machines can develop an understanding of text but not necessarily how that text was generated. In the words of Grimmer and Stewart (Political Analysis, 2013), all models of natural language are wrong but some are useful.

For now, all that's important to understand about this issue is that text analysis does not tell researchers much about the generation of the text itself, but we can use these tools to infer topics, actors, important terms and patterns within text. For most work in political science, this is enough.

## Tokens

Lets understand the way we manipulate text so that it is easier for us to study. This is an important step in text analysis often referred to as "pre-processing". Like I said before, strings are dumb and not particularly helpful on their own. We have to format the text into a shape that makes it easier to study computationally. In this example I'm using a series of functions from a package called `tidytext` which is based on the `tidyverse` approach to data analysis in `R`.

The best way to study texts is word by word. This means that we need a data frame format that treats each word in a text like its an observation of a variable. In the text analysis literature, individual words are called `tokens` and the pre-processing of words into a token format is called `tokenization` (sometimes it is referred to as "lemmatization" - the linguist term for a word's dictionary form).

I borrow this next example from the book *Tidy Text Mining with R* by Julia Silge and David Robinson.

Lets take this Emily Dickinson poem:

```{r dickinson}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
text
```

Now, we want to turn each word into an observation of a variable. This way each word is its own row in a data frame.

We can do this easily with `dplyr` and `tidytext` by making a tibble (the tidyverse version of a data frame) and then tokenizing each line of the poem.

First we make a data frame out of the text.
```{r}
library(dplyr)
text_df <- tibble(line = 1:4, text = text)
```
```{r echo=FALSE}
knitr::kable(text_df)
```

And now we use `unnest_tokens()` to break the text up by word.

```{r}
library(tidytext)
text_df <- text_df %>% # the %>% symbol is called a pipe - pipes are a simple way of passing an object through a function in a readable way. Think of the object text_df being piped into the function unnest_tokens
  unnest_tokens(word, text)
```

```{r, echo=FALSE}
knitr::kable(text_df)
```

The arguments `word` and `text` in the `unnest_tokens` function are referring to the column names from our dataset.

With the above function, we have created a data frame that has one observation (row) for each word in the text. This makes it easier to actually process, visualize or study the text as a whole.

## Examining Multiple Documents
Lets look at some books for example. Using the package `gutenbergr` we can download the text from any gutenberg project book into R. Here, I've downloaded all of Thoreau's works in the library and turned them all into a tidy dataframe that we can then analyze!

```{r}
#devtools::install_github("ropensci/gutenbergr")
library(gutenbergr)
thoreau <- gutenberg_works(author == "Thoreau, Henry David")

id <- thoreau$gutenberg_id

#thoreau_works_df <- gutenberg_download(id)

thoreau_works_df <- readRDS(file = "thoreau_works_df.rds")

thoreau_works_df <- thoreau_works_df %>% 
  left_join(.,thoreau, by = "gutenberg_id")
```

As you can see, the `thoreau` object is a data frame where each line from a book is connected to that book's id. Just like the poem before, we can break the documents up into words for easier analysis and counts. We can also get a simple count of the top words in all of Thoreau's work.

```{r}
thoreau_works_tidy <- thoreau_works_df %>% 
  unnest_tokens(word, text)

thoreau_works_tidy %>% count(word, sort = T)
```

We can also use the group_by function to separate the counts by book.

```{r}
thoreau_works_tidy %>% group_by(gutenberg_id) %>% count(word, sort = T)
```

Uh oh! Have you noticed that the top words are all super boring or uninformative? These are terms we call `stop words`. They're grammatically important but usually they are substantively unimportant - especially for understanding the topics of certain texts. We can easily remove them in a variey of ways but I tend to use the `anti_join` function which will remove them from the dataset in one line of code. There are other ways of doing this as well that I may touch on later. But for now we can simply write.

## Cleaning up Text
```{r}
library(tidyr)
thoreau_works_tidy <- thoreau_works_tidy %>% 
  anti_join(stop_words) 

thoreau_works_tidy %>% count(word, sort = T)
```

That looks a lot better! But there's a number, 6, third on the list. Usually we want to remove numbers and special characters because we're interested purely in the substantive terms. Numbers, out of context, won't usually help us. So we can get rid of numbers, punctuation and special characters as well. This is a common step in text analysis referred to as **pre-processing**. The end result of pre-processing should be an object sometimes referred to as a "corpus" or a collection of texts in a form that is basically ready for analysis.

There a number of different ways to pre-process the data. The most useful involves a set of base functions in R combined with a set of terms called *regular expressions* (regex).

### grep, Regex and Pre-Processing
The set of functions are `grep()` and `grepl()`. These two functions search through your data for patterns in character strings. Using `grep` will give you an index of where in the data the pattern of text is found. `grepl` is a logical function, so it returns a TRUE/FALSE statement for each row depending on if there is a pattern match.

Another powerful set of functions that follow from `grep` and `grepl` are `sub()` and `gsub()`. These functions will search for a character vector that matches a pattern and then replace it with something else.

Regex can look confusing or be obtuse if you don't know what you're looking for. If there's a specific problem with the text that you need to fix, consider looking at this regex cheat-sheet (https://www.rexegg.com/regex-quickstart.html).

But, for the general case of removing numbers and punctuation, the following code works. The first line looks for one or more digits (the `\d+` means one or more digits). The second removes special characters (the `^` symbol says to look for characters that are *not* `[:alnum:]`, which stands for "numbers and letters"). Finally, I remove thoreaus' name because I know it will appear but the term isn't useful for analysis.

```{r}
thoreau_works_tidy <- thoreau_works_tidy[-grep("\\b\\d+\\b", thoreau_works_tidy$word),]
thoreau_works_tidy <- thoreau_works_tidy[-grep("[^[:alnum:]]", thoreau_works_tidy$word),]
thoreau_works_tidy <- thoreau_works_tidy[-grep("thoreau", thoreau_works_tidy$word),]
thoreau_works_tidy %>% count(word, sort = T)
```
For some types of word analysis it is useful to convert the corpus object into a *sparse matrix* where the rows are documents and the columns are a variable for each unique word in the corpus. The observations of each column are the count of a word appearing in a specific document. If we were to do this with the Thoreau works, we could conduct some cross-work analysis as well as easier visualization.

```{r}
thoreau_dfm <-
  thoreau_works_tidy %>%
  count(gutenberg_id, word) %>%
  cast_dfm(gutenberg_id, word, n)

head(thoreau_dfm)
```

We can visualize text easily with a graphic like a wordcloud to get a better picture of the documents.

```{r}
library(quanteda)

set.seed(123)

textplot_wordcloud(thoreau_dfm, max_words = 100,  color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
```

These are the basics of handling text data. Next, I want to focus on a case of text that is often examined in political science, social media posts. The prevalence of social media posts that reveal political opinions, relay news events, and spread fake news has turned the study of social media into an important piece of political communication literature. Luckily, with a knowledge of the basics we can start to analyze tweets just like we did Thoreau (although you'll likely find that Thoreau accomplishes more rhetorically in 140 characters or less than almost anyone who posts online today).
